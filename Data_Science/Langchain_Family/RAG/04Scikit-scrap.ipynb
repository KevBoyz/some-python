{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8e7465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3437b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows11\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-llm-9c8IAL59-py3.13\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323c2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://scikit-learn.org/stable/'\n",
    "flat_documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f86d1d",
   "metadata": {},
   "source": [
    "## User guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0885d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links of User guide\n",
    "\n",
    "user_guide = BeautifulSoup(\n",
    "    requests.get('https://scikit-learn.org/stable/user_guide.html').text\n",
    ")\n",
    "links = []\n",
    "\n",
    "\n",
    "links_navbar = user_guide.find(class_='nav bd-sidenav')\n",
    "all_li = links_navbar.find_all('li')\n",
    "\n",
    "for li in all_li:\n",
    "    details = li.find('details')\n",
    "    if details:\n",
    "        ul = details.find('ul')\n",
    "        anchors = ul.find_all('a')\n",
    "        for a in anchors:\n",
    "            links.append(a.get('href'))\n",
    "    else:\n",
    "        anchor = li.find('a')\n",
    "        links.append(anchor.get('href'))\n",
    "\n",
    "\n",
    "responses = [requests.get(base_url + link).text for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap each page\n",
    "\n",
    "headers = [  \n",
    "    ('h1', 'Page'),\n",
    "    ('h2', 'Section'),\n",
    "    ('h3', 'Sub Section'),\n",
    "    ('h4', 'Sub Section'),\n",
    "]\n",
    "\n",
    "splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers,\n",
    ")\n",
    "\n",
    "documents = [splitter.split_text(r) for r in responses]\n",
    "\n",
    "for html_split in documents:  # Desempacota as listas\n",
    "    for chunk in html_split:\n",
    "        if chunk.metadata and len(chunk.page_content) > 50:  # Deixa de incluir chunks bugados e títulos das páginas.\n",
    "            flat_documents.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da4b65",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8f5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = BeautifulSoup(\n",
    "    requests.get('https://scikit-learn.org/stable/api/index.html').text\n",
    ")\n",
    "links = []\n",
    "\n",
    "\n",
    "tbody = api.find('tbody')\n",
    "all_tr = tbody.find_all('tr')\n",
    "\n",
    "for tr in all_tr:\n",
    "    td = tr.find('td')\n",
    "    link = td.find('a').get('href')\n",
    "    links.append(link[3:])\n",
    "\n",
    "\n",
    "responses = [requests.get(base_url + link).text for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc8024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[r'\\n\\s*\\n+'],\n",
    "    is_separator_regex=True,\n",
    "    chunk_overlap=0,\n",
    "    chunk_size=1000\n",
    "    )\n",
    "\n",
    "split_of_pages = []\n",
    "\n",
    "for r in responses: \n",
    "    soup = BeautifulSoup(r)\n",
    "    content = soup.find('section')\n",
    "    \n",
    "    page_name = content.find('h1').text[:-1]\n",
    "\n",
    "\n",
    "    try:\n",
    "        parameters = content.find('dt').text.replace('[source]#', '')\n",
    "        description = content.find('dd').text\n",
    "    except AttributeError:  # Algumas páginas parecem não ter esses elementos\n",
    "        continue\n",
    "\n",
    "\n",
    "    page_content = f'{page_name}\\n{parameters}\\n{description}'\n",
    "    splits = splitter.split_text(page_content)\n",
    "\n",
    "    for chunk in splits:\n",
    "        split_of_pages.append(\n",
    "            Document(page_content=chunk, metadata={\"page\": page_name})\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dadebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in split_of_pages:\n",
    "    flat_documents.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba596a7",
   "metadata": {},
   "source": [
    "## Make embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(\n",
    "    flat_documents, embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c65487",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type='similarity_score_threshold',\n",
    "    search_kwargs={\"k\": 5, 'score_threshold': 0.2} # Acima de 0.2 parece não funcionar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided below, including the source of the information at the end of your response.\n",
    "      \n",
    "Context:                                   \n",
    "{context}\n",
    "\n",
    "User input:\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "rag_chain = ( \n",
    "    {\"context\": retriever, \"question\": RunnableLambda(lambda x: x)}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(\"How to use linear regression in scikit learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9845650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:\n",
      "To use linear regression in scikit-learn, you can follow these steps:\n",
      "\n",
      "1. Import the necessary libraries:\n",
      "   ```python\n",
      "   import numpy as np\n",
      "   from sklearn.linear_model import LinearRegression\n",
      "   ```\n",
      "\n",
      "2. Prepare your dataset (features and target). For example, you can create a synthetic dataset using `make_regression`:\n",
      "   ```python\n",
      "   from sklearn.datasets import make_regression\n",
      "   X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
      "   ```\n",
      "\n",
      "3. Create a Linear Regression model:\n",
      "   ```python\n",
      "   model = LinearRegression()\n",
      "   ```\n",
      "\n",
      "4. Fit the model to your data:\n",
      "   ```python\n",
      "   model.fit(X, y)\n",
      "   ```\n",
      "\n",
      "5. Make predictions:\n",
      "   ```python\n",
      "   predictions = model.predict(X)\n",
      "   ```\n",
      "\n",
      "6. Evaluate the model (optional):\n",
      "   ```python\n",
      "   score = model.score(X, y)\n",
      "   print(f\"Model Score: {score}\")\n",
      "   ```\n",
      "\n",
      "This is a basic example of how to use linear regression in scikit-learn. You can adjust the parameters and the dataset according to your specific needs.\n",
      "\n",
      "(Source: scikit-learn documentation on Linear Regression\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-llm-9c8IAL59-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
